# Evaluation Metrics for Image and Text Similarity

This repository contains code to evaluate image and text similarity using OpenAI's CLIP model, as well as perceptual similarity between images using LPIPS (Learned Perceptual Image Patch Similarity). The evaluation is conducted on two different pipelines: `DreamFusion` and `OurPipeline`. The metrics computed include cosine similarity (using CLIP) and perceptual similarity (using LPIPS).

## Prerequisites

Before running the code, ensure you have the following installed:

- `torch`: PyTorch library for machine learning.
- `torchvision`: A library for image transformations.
- `clip`: OpenAI's CLIP model.
- `numpy`: A fundamental package for numerical computation.
- `PIL`: Python Imaging Library for image processing.
- `lpips`: Library for computing perceptual image similarity.

You can install the required dependencies using:

```bash
!pip install git+https://github.com/openai/CLIP.git
!pip install torch torchvision
!pip install numpy pillow lpips
```

## CLIP-Based Cosine Similarity

The code evaluates image-text similarity using CLIP's cosine similarity between the text prompts and images. It calculates similarity for both front and back views of the images generated by `DreamFusion` and `OurPipeline`.

### Steps:
1. **Setup**: The CLIP model is loaded, and GPU support is enabled if available.
2. **Dataset Structure**: Images are stored in subfolders under `/dreamfusion/` and `/pipeline/` for different prompts.
3. **Processing**: 
   - Images are loaded and preprocessed.
   - Text tokens are generated using the given prompts.
   - The cosine similarity between the encoded images and text is computed for front and back views.
4. **Results**: For each prompt, the average cosine similarity between the text and the images is printed for both `DreamFusion` and `OurPipeline`.

## Pairwise Cosine Similarity Between Images

The code also computes the cosine similarity between front and back views of the same object generated by `DreamFusion` and `OurPipeline` using CLIP embeddings.

### Steps:
1. **Processing**: The front and back views of the images are loaded and preprocessed.
2. **Cosine Similarity Calculation**: The cosine similarity between the front and back images is computed.
3. **Results**: For each prompt, the pairwise cosine similarity for front vs. back images is printed.

## LPIPS Perceptual Similarity

In addition to CLIP-based cosine similarity, LPIPS is used to compute perceptual similarity between the images generated by `DreamFusion` and `OurPipeline`.

### Steps:
1. **LPIPS Model Setup**: The LPIPS model (`alex` version) is loaded.
2. **Image Preprocessing**: Images are resized to 256x256 before evaluation.
3. **Perceptual Similarity Calculation**: LPIPS computes the perceptual similarity score between the front and back images of both pipelines.
4. **Results**: For each prompt, LPIPS scores are printed for both front and back views.

## Directory Structure

The images for evaluation should be structured as follows:

```
/data
  ├── dreamfusion
  │   ├── <prompt_1>
  │   │   ├── front.png
  │   │   ├── back.png
  │   ├── <prompt_2>
  │   │   ├── front.png
  │   │   ├── back.png
  │   └── ...
  └── ourpipeline
      ├── <prompt_1>
      │   ├── front.jpg
      │   ├── back.jpg
      ├── <prompt_2>
      │   ├── front.jpg
      │   ├── back.jpg
      └── ...
```

# Prompts

The evaluation is based on the names of the folders in the dataset. For example:

```python
prompts = [
    "a crab, low poly",
    "a bald eagle carved out of wood",
    "a ripe strawberry",
    "a silver platter piled high with fruits",
    "an ice cream sundae"
]
```
Each folder name in /dreamfusion/ and /pipeline/ directories serves as the prompt for the corresponding images.


## Usage

1. Mount Google Drive to access the dataset:
   ```python
   from google.colab import drive
   drive.mount('/content/drive')
   ```

2. Run the evaluation code to compute and print the results for each metric.

## Contact

For any issues or queries, feel free to reach out or create an issue in the repository.